{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475c448b",
   "metadata": {},
   "source": [
    "Views Notebook (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e319678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a local Spark cluster with two executors (if it doesn't already exist)\n",
    "spark_session = SparkSession.builder.master('local[2]').getOrCreate()\n",
    "sc = spark_session.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e719f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window, to_json, struct\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Load cleaned stream from Kafka \n",
    "cleaned_stream = (\n",
    "    spark_session.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"ingest-cleaned\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json\")\n",
    "    .selectExpr(\"from_json(json, 'timestamp TIMESTAMP') as data\")\n",
    "    .select(\"data.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f32c7",
   "metadata": {},
   "source": [
    "Events per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be073066",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_counts = (\n",
    "    cleaned_stream\n",
    "    .withWatermark(\"timestamp\", \"30 minutes\")\n",
    "    .groupBy(window(col(\"timestamp\"), \"1 day\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "tq = (\n",
    "    daily_counts.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"events_per_day\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b9c17",
   "metadata": {},
   "source": [
    "To show/display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark_session.sql(\"SELECT * FROM events_per_day\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23c804",
   "metadata": {},
   "source": [
    "Part 2 Events per hour with delta <-> previous day  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the file for the static dataframe\n",
    "# Define the file path\n",
    "file_path = \"data/historical_hourly.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "     # Write an empty CSV file with only the header\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(\"hour,historical_count\\n\")\n",
    "    print(f\"Empty file created at {file_path}\")\n",
    "else:\n",
    "    print(f\"File already exists at {file_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c0db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load static historical data\n",
    "historical_df = (\n",
    "    spark_session.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"data/historical_hourly.csv\")\n",
    "    .withColumn(\"hour\", col(\"hour\").cast(TimestampType()))\n",
    "    .withColumn(\"historical_count\", col(\"historical_count\").cast(\"long\"))\n",
    ")\n",
    "\n",
    "# Compute current hourly counts\n",
    "hourly_counts = (\n",
    "    cleaned_stream\n",
    "    .withWatermark(\"timestamp\", \"30 minutes\")\n",
    "    .groupBy(window(col(\"timestamp\"), \"1 hour\"))\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"current_count\")\n",
    "    .withColumn(\"hour\", col(\"window.start\"))\n",
    ")\n",
    "\n",
    "# Join with historical data and compute delta\n",
    "hourly_with_delta = (\n",
    "    hourly_counts\n",
    "    .join(historical_df, on=\"hour\", how=\"left\")\n",
    "    .withColumn(\"delta\", col(\"current_count\") - col(\"historical_count\"))\n",
    "    .select(\"hour\", \"current_count\", \"historical_count\", \"delta\")\n",
    ")\n",
    "\n",
    "# Write to Kafka topic \"hourly-delta\"\n",
    "output_to_kafka = (\n",
    "    hourly_with_delta\n",
    "    .select(to_json(struct(\"hour\", \"current_count\", \"historical_count\", \"delta\")).alias(\"value\"))\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"hourly-delta\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints-hourly-delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277f290",
   "metadata": {},
   "source": [
    "Wait for stream to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d82c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_to_kafka.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
