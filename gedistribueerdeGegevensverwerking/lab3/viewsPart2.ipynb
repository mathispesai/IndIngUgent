{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475c448b",
   "metadata": {},
   "source": [
    "Views Notebook (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e719f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window, to_json, struct\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Load cleaned stream from Kafka \n",
    "cleaned_stream = (\n",
    "    spark_session.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"ingest-cleaned\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json\")\n",
    "    .selectExpr(\"from_json(json, 'timestamp TIMESTAMP') as data\")\n",
    "    .select(\"data.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f32c7",
   "metadata": {},
   "source": [
    "Events per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be073066",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_counts = (\n",
    "    cleaned_stream\n",
    "    .withWatermark(\"timestamp\", \"30 minutes\")\n",
    "    .groupBy(window(col(\"timestamp\"), \"1 day\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "tq = (\n",
    "    daily_counts.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"events_per_day\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b9c17",
   "metadata": {},
   "source": [
    "To show/display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark_session.sql(\"SELECT * FROM events_per_day\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23c804",
   "metadata": {},
   "source": [
    "Part 2 Events per hour with delta <-> previous day  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c0db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load static historical data\n",
    "historical_df = (\n",
    "    spark_session.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"data/historical_hourly.csv\")\n",
    "    .withColumn(\"hour\", col(\"hour\").cast(TimestampType()))\n",
    "    .withColumn(\"historical_count\", col(\"historical_count\").cast(\"long\"))\n",
    ")\n",
    "\n",
    "# Compute current hourly counts\n",
    "hourly_counts = (\n",
    "    cleaned_stream\n",
    "    .withWatermark(\"timestamp\", \"30 minutes\")\n",
    "    .groupBy(window(col(\"timestamp\"), \"1 hour\"))\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"current_count\")\n",
    "    .withColumn(\"hour\", col(\"window.start\"))\n",
    ")\n",
    "\n",
    "# Join with historical data and compute delta\n",
    "hourly_with_delta = (\n",
    "    hourly_counts\n",
    "    .join(historical_df, on=\"hour\", how=\"left\")\n",
    "    .withColumn(\"delta\", col(\"current_count\") - col(\"historical_count\"))\n",
    "    .select(\"hour\", \"current_count\", \"historical_count\", \"delta\")\n",
    ")\n",
    "\n",
    "# Write to Kafka topic \"hourly-delta\"\n",
    "output_to_kafka = (\n",
    "    hourly_with_delta\n",
    "    .select(to_json(struct(\"hour\", \"current_count\", \"historical_count\", \"delta\")).alias(\"value\"))\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"topic\", \"hourly-delta\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints-hourly-delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277f290",
   "metadata": {},
   "source": [
    "Wait for stream to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d82c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_to_kafka.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
